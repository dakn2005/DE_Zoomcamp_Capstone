id: plane_crash_data
namespace: zoomcamp
description: |
  capstone project for plane-crashes analysis 
  source: https://www.planecrashinfo.com/database.htm

# inputs:
#   - id: year
#     type: SELECT
#     displayName: Select year
#     values: "{{ range(1920, 2026) | list }}"
#     defaults: 2024

variables:
  file: "{{trigger.date | date('yyyy')}}.csv"
  gcs_file: "gs://{{kv('GCP_BUCKET_NAME_4')}}/csvs/{{vars.file}}"
  data: "{{outputs.extract.outputFiles[(trigger.date | date('yyyy')) ~ '.csv']}}"

tasks:
  - id: set_label
    type: io.kestra.plugin.core.execution.Labels
    labels:
      file: "{{render(vars.file)}}"
      # taxi: "{{inputs.taxi}}"

  - id: extract
    type: io.kestra.plugin.scripts.python.Script
    # containerImage: python:3.11-alpine
    runner: DOCKER
    beforeCommands:
      - pip install beautifulsoup4 lxml pandas
    outputFiles:
      - "{{render(vars.file)}}"
    # requirements:
    #   - beautifulsoup4
    #   - lxml
    #   - pandas

    script: |
      from io import StringIO
      import requests
      from bs4 import BeautifulSoup
      import pandas as pd
      from concurrent.futures import ThreadPoolExecutor
      import time
      import json

      def replace_key(key: str):
          key = key.replace(":", "").replace("#", "").replace(" ", "").lower()

          match key:
              case "ac\ntype":
                  return "actype"
              case "cn/ln":
                  return "cn_ln"
              case _:
                  return key

      def record_page(url):
          response = requests.get(url)
          soup = BeautifulSoup(response.text, "html.parser")

          # Find the first table
          table = soup.find("table")

          # Extract rows
          if not table:
              return dict([])

          rows = []
          for tr in table.find_all("tr")[1:]:
              cells = [td.text.strip() for td in tr.find_all("td")]
              if cells:
                  cells[0] = replace_key(cells[0])
                  cells[1] = str(cells[1]).replace("\xa0", "")
                  if cells[0] == "date":
                      cells[1] = f"{cells[1]}"

                  rows.append(cells)

          return dict(rows)

      def get_rec(year):
          base_url = f"https://www.planecrashinfo.com/{year}"
          # Fetch the HTML content
          url = f"{base_url}/{year}"
          response = requests.get(
              f"{url}.htm",
              headers={
                  "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
              },
          )
          # response = requests.get('https://example.com')
          
          soup = BeautifulSoup(response.text, "html.parser")

          # Find the first table
          table = soup.find("table")

          # print(year, table)

          if table is None:
              return None

          # Extract headers
          headers = [td.text.strip() for td in table.find_all("tr")[0]]

          # Extract rows
          rows = []
          cnt = 0
          for tr in table.find_all("tr")[1:]:  # Skip header row
              cells = [td.text.strip() for td in tr.find_all("td")]
              if cells:
                  rows.append(cells)
                  cnt += 1

          # # Convert to DataFrame
          # df = pd.DataFrame(rows, columns=headers)

          # # Display DataFrame
          # df.head()

          urls = []
          for i in range(1, cnt + 1):
              urls.append(f"{url}-{i}.htm")

          # results = record_page(urls[0])

          with ThreadPoolExecutor() as executor:
              results = executor.map(record_page, urls)

          # print(results)
          return results

      yr = {{trigger.date | date('yyyy')}}
      arr = []
      # start = time.time()
      records = get_rec(yr)

      # time.sleep(20)

      if records is None:
          print(f"failed: {yr}")
      else:
          for rec in records:
              # print(rec)
              arr.append(rec)

          json_string = json.dumps(arr)
          df = pd.read_json(StringIO(json_string))

          df['summary'] = df['summary'].str.replace(r'[^a-zA-Z0-9\s\.]', '', regex=True)

          df.to_csv(
              "{{render(vars.file)}}",
              index=False,
              sep='|'
          )

          # print("total time {:.2f}s".format(time.time() - start))
          # print(arr[0])
          # df.head()

          # df.to_json(
          #     "{{render(vars.file)}}",
          #     orient="records",
          #     # lines=True
          # )

          print("success: {{render(vars.file)}}")

  - id: upload_to_gcs
    type: io.kestra.plugin.gcp.gcs.Upload
    from: "{{render(vars.data)}}"
    to: "{{render(vars.gcs_file)}}"

  - id: purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
    description: To avoid cluttering your storage, we will remove the downloaded files

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{kv('GCP_CREDS')}}"
      projectId: "{{kv('GCP_PROJECT_ID')}}"
      location: "{{kv('GCP_LOCATION')}}"
      bucket: "{{kv('GCP_BUCKET_NAME_4')}}"

triggers:
  - id: scheduler
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "@yearly"
    # cron: "0 9 1 * *"